{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosted Trees\n",
    "\n",
    "\n",
    "Este notebook, by [Felipe Alonso Atienza](www.linkedin.com/in/felipe-alonso-atienza)\n",
    "\n",
    "Vamos a analizar el funcionamiento de los métodos de Gradient Boosting de sklearn mediante ejemplos ilustrativos. \n",
    "\n",
    "## Contenidos\n",
    "\n",
    "1. Un problema de clasificación\n",
    "2. Un problema de regresión\n",
    "\n",
    "## Librerías y funciones\n",
    "\n",
    "Lo primero es cargar las librerías y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Un problema de clasificación\n",
    "\n",
    "Utilizaremos el [Pima Indian Diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database).\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Cargue los datos almacenados en el fichero *diabetes.csv*\n",
    "</div>\n",
    "\n",
    "### Cargamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu código aquí\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis exploratorio básico\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Represente el histograma de cada variable separando por clases, ¿hay alguna característica que convenga transformar?\n",
    "Nota: Si lo considera de utilidad, reutilice el código del Notebook anterior.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos histogramas para cada clase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: A la vista de los histogramas anteriores, ¿cómo de separable crees que es el problema?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyamos nuestro conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preparamos los datos\n",
    "features = data.columns.drop(['Outcome'])\n",
    "X = data[features].as_matrix()\n",
    "y = data['Outcome'].as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "print('Datos train: ', X_train.shape)\n",
    "print('Datos test:  ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "\n",
    "Probemos primero con un árbol de decisión sencillo\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Entrena un árbol de decisión sencillo, y muestra las prestaciones para el conjunto de test.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pintemos el árbol, a ver qué variables no salen más relevantes:\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Representa el árbol entrenado\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece un problema complicado de clasificar, porque la profundidad del árbol óptima es 2. El algoritmo está bien entrenado porque la diferencia entre train y test es pequeña. Si queremos mejorar prestaciones tendremos que acudir a un modelo más complejo.\n",
    "\n",
    "### Boosted Trees\n",
    "\n",
    "El entrenamiento del algoritmo de boosting requiere fijar tres parámetros libres:\n",
    "\n",
    "- Número de iteraciones\n",
    "- Tasa de aprendizaje ($\\alpha$)\n",
    "- Complejidad del árbol: *max_depth*\n",
    "\n",
    "Se podría hacer una búsqueda sobre los tres parámetros conjuntamente mediante GridSearchCV, sin embargo, es muy costoso computacionalmente, con lo que es más sencillo aplicar una optimización secuencial: se prueban distintos valores de los parámetros libres, se fijan los óptimos y se busca sobre el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "Niterations = [25,50,75,100,125,150,175,200,300]\n",
    "learningRate = [0.5,0.1,0.05,0.01]\n",
    "# mantenemos max_depth estático: max_depth=2\n",
    "\n",
    "param_grid = {'n_estimators': Niterations,'learning_rate':learningRate }\n",
    "grid = GridSearchCV(GradientBoostingClassifier(random_state=0, max_depth=2), param_grid=param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representemos el error que estamos cometiendo para los distintos valores de los parámetros libres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos métricas globales\n",
    "bt = GradientBoostingClassifier(random_state=0, max_depth=2,learning_rate= 0.05, n_estimators= 50)\n",
    "bt.fit(X_train,y_train)\n",
    "\n",
    "error = 1-grid.cv_results_['mean_test_score'].reshape(len(learningRate),len(Niterations))\n",
    "colors = ['r','b','g','k','m']\n",
    "for i,lr in enumerate(learningRate):    \n",
    "    plt.plot(Niterations,error[i,:],colors[i] + '--o',label='lr = %g'%lr)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('# iteraciones')\n",
    "plt.ylabel('5-fold CV Error')\n",
    "plt.title('train: %0.3f\\ntest:  %0.3f'%(bt.score(X_train,y_train),bt.score(X_test,y_test)))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las prestaciones no son mucho mejores que con respecto a un árbol sencillo. Como el coste de entrenamiento de este conjunto no es muy grande, replicaremos el análisis anterior aumentando el valor de la complejidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Niterations = [25,50,75,100,125,150,175,200,300]\n",
    "learningRate = [0.5,0.1,0.05,0.01]\n",
    "# mantenemos max_depth estático: max_depth=3\n",
    "\n",
    "param_grid = {'n_estimators': Niterations,'learning_rate':learningRate }\n",
    "grid = GridSearchCV(GradientBoostingClassifier(random_state=0, max_depth=3), param_grid=param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos métricas globales\n",
    "bt = GradientBoostingClassifier(random_state=0, max_depth=3,learning_rate= 0.01, n_estimators= 125)\n",
    "bt.fit(X_train,y_train)\n",
    "\n",
    "error = 1-grid.cv_results_['mean_test_score'].reshape(len(learningRate),len(Niterations))\n",
    "colors = ['r','b','g','k','m']\n",
    "for i,lr in enumerate(learningRate):    \n",
    "    plt.plot(Niterations,error[i,:],colors[i] + '--o',label='lr = %g'%lr)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('# iteraciones')\n",
    "plt.ylabel('5-fold CV Error')\n",
    "plt.title('train: %0.3f\\ntest:  %0.3f'%(bt.score(X_train,y_train),bt.score(X_test,y_test)))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que al aumentar la complejidad, necesitamos una tasa de aprendizaje más pequeña. En general, cuanto más complejo es el problema, menor es la tasa de aprendizaje y mayor el número de iteraciones que necesita el algoritmo. Parece que podemos ir un poco más allá, disminuyamos un poco más la tasa de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Niterations = [25,50,75,100,125,150,175,200,300]\n",
    "learningRate = [0.5,0.1,0.05,0.01,0.005]\n",
    "# mantenemos max_depth estático: max_depth=3\n",
    "\n",
    "param_grid = {'n_estimators': Niterations,'learning_rate':learningRate }\n",
    "grid = GridSearchCV(GradientBoostingClassifier(random_state=0, max_depth=3), param_grid=param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos métricas globales\n",
    "bt = GradientBoostingClassifier(random_state=0, max_depth=3,learning_rate= 0.005, n_estimators= 200)\n",
    "bt.fit(X_train,y_train)\n",
    "\n",
    "error = 1-grid.cv_results_['mean_test_score'].reshape(len(learningRate),len(Niterations))\n",
    "colors = ['r','b','g','k','m']\n",
    "for i,lr in enumerate(learningRate):    \n",
    "    plt.plot(Niterations,error[i,:],colors[i] + '--o',label='lr = %g'%lr)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('# iteraciones')\n",
    "plt.ylabel('5-fold CV Error')\n",
    "plt.title('train: %0.3f\\ntest:  %0.3f'%(bt.score(X_train,y_train),bt.score(X_test,y_test)))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar también la importancia de las variables\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Representa la importancia de las variables\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también podemos representar la dependencia de cada característica con la variable target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "fig, ax = plot_partial_dependence(bt, X_train, indices, feature_names=features,\n",
    "                                 percentiles=(0.0, 1.0), n_cols = 4)\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Un problema de regresión\n",
    "\n",
    "Volvemos a nuestro conjunto de datos ya conocido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos datos\n",
    "house_data = pd.read_csv(\"./data/kc_house_data.csv\") # cargamos fichero\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "\n",
    "# convertir las variables en pies al cuadrado en metros al cuadrado \n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\n",
    "\n",
    "# renombramos\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# añadimos las nuevas variables\n",
    "house_data['years']            = 2017 - house_data['yr_built']\n",
    "house_data['bedrooms_squared'] = house_data['bedrooms'].apply(lambda x: x**2)\n",
    "house_data['bed_bath_rooms']   = house_data['bedrooms']*house_data['bathrooms']\n",
    "house_data['log_sqm_living']   = house_data['sqm_living'].apply(lambda x: np.log(x))\n",
    "house_data['lat_plus_long']    = house_data['lat']*house_data['long']\n",
    "\n",
    "# convertimos el DataFrame al formato necesario para scikit-learn\n",
    "data = house_data.as_matrix() \n",
    "\n",
    "y = data[:,0:1]     # nos quedamos con la 1ª columna, price\n",
    "X = data[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "feature_names = house_data.columns[1:]\n",
    "\n",
    "# Dividimos los datos en entrenamiento y test (80 training, 20 test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state = 2)\n",
    "\n",
    "print('Datos entrenamiento: ', X_train.shape)\n",
    "print('Datos test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el número de muestras del problema es mayor, y para no extender el entrenamiento, escogeremos de forma muy secuencial los parámetros libres.\n",
    "\n",
    "AVISO: Este proceso va a tardar un rato ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "Niterations = [500,1000,1500,2000]\n",
    "learningRate = [0.1,0.05]\n",
    "# mantenemos max_depth estático: max_depth=3\n",
    "\n",
    "param_grid = {'n_estimators': Niterations,'learning_rate':learningRate }\n",
    "grid = GridSearchCV(GradientBoostingRegressor(random_state=0, max_depth=3), param_grid=param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = GradientBoostingRegressor(random_state=0, max_depth=3,learning_rate= 0.1, n_estimators= 1500)\n",
    "bt.fit(X_train,y_train)\n",
    "\n",
    "error = 1-grid.cv_results_['mean_test_score'].reshape(len(learningRate),len(Niterations))\n",
    "colors = ['r','b','g','k','m']\n",
    "for i,lr in enumerate(learningRate):    \n",
    "    plt.plot(Niterations,error[i,:],colors[i] + '--o',label='lr = %g'%lr)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('# iteraciones')\n",
    "plt.ylabel('5-fold CV Error')\n",
    "plt.title('train: %0.3f\\ntest:  %0.3f'%(bt.score(X_train,y_train),bt.score(X_test,y_test)))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importancia y dependencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bt.feature_importances_\n",
    "importances = importances / np.max(importances)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(range(X_train.shape[1]),importances[indices])\n",
    "plt.yticks(range(X_train.shape[1]),feature_names[indices])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, representemos los conocidos como *partial dependence plots*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "fig, ax = plot_partial_dependence(bt, X_train, indices, feature_names=feature_names,\n",
    "                                 percentiles=(0.0, 1.0), n_cols = 5)\n",
    "fig.set_size_inches(20, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What's next?\n",
    "\n",
    "Así que, ¿qué más podríamos hacer? aumentar *max_depth*, pero seguramente incurramos en overfitting. Aquí radica el arte del *machine learning*:\n",
    "\n",
    "1. Conoce el problema (negocio), para generar nuevas *features* de interés\n",
    "2. Trata de buscar más ejemplos para utilizar técnicas de aprendizaje profundo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
