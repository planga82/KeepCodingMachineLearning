{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging y Random Forest\n",
    "\n",
    "\n",
    "Este notebook, by [Felipe Alonso Atienza](www.linkedin.com/in/felipe-alonso-atienza)\n",
    "\n",
    "Vamos a analizar el funcionamiento de los métodos de Bagging ([clasificación](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)/[regresión](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)) y de Random Forest ([clasificación](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)/[regresión](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)) mediante ejemplos ilustrativos. \n",
    "\n",
    "## Contenidos\n",
    "\n",
    "1. Ejemplo en clasificación\n",
    "2. Ejemplo en regresión\n",
    "\n",
    "## Librerías y funciones\n",
    "\n",
    "Lo primero es cargar las librerías y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ejemplo en clasificación\n",
    "\n",
    "En este primer ejemplo vamos a explorar el conjunto de datos para la detección de cancer de mama ([Breast Cancer](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)). \n",
    "\n",
    "El objetivo es detectar si un cancer es beningo o maligno (B/N) a partir de la información de atributos numéricos que caracterizan los núcleos celulares de las imágenes digitalizadas de biopsias realizadas a distintos pacientes. \n",
    "\n",
    "La variable target es *diagnosis*, mientras que *id* es irrelevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Cargue los datos almacenados en el fichero *breast_cancer.csv* y elimine las columnas *id* y *Unnamed: 32*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vemos cómo se distribuye la variable *diagnosis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos ahora *diagnosis* en una variable numérica. Aquí tenemos varias opciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# opción 1. np.where\n",
    "data['label_opcion1'] = np.where(data['diagnosis'] == 'M',1,0)\n",
    "\n",
    "\n",
    "# opción 2. LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['diagnosis'])\n",
    "\n",
    "print(list(le.classes_))\n",
    "\n",
    "data['label_opcion2'] = le.transform(data['diagnosis'])\n",
    "\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar conviene echar un vistazo a los datos. Como todos los datos son numéricos, un histograma puede ser una buena opción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_table('./data/breast_cancer.csv',sep=',', decimal='.')\n",
    "data =  data.drop(['id','Unnamed: 32'],axis=1)\n",
    "data['diagnosis'] = np.where(data['diagnosis'] == 'M',1,0)\n",
    "\n",
    "# Pintamos histogramas para cada clase\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "idx_0 =  data['diagnosis'] == 0\n",
    "idx_1 =  data['diagnosis'] == 1\n",
    "\n",
    "for i,feature in enumerate(data.columns.drop(['diagnosis'])):\n",
    "    plt.subplot(6,5,i+1)   \n",
    "    plt.hist(data.ix[idx_0,feature],normed=1, alpha=0.75,label='y=0')\n",
    "    plt.hist(data.ix[idx_1,feature],normed=1, facecolor='red', alpha=0.75,label='y=1')\n",
    "    plt.legend()\n",
    "    plt.title(feature)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de los histogramas anteriores, podemos hacer transformaciones sobre las variables, para que el histograma se parezca más a una gaussiana. Esto se conoce como transformación de variables, y no modifican el resultado de la clasificación. Cuando tenemos distribuciones asimétricas (como por ejemplo *area_se*), podemos aplicar una transformación logarítmica o raíz cuadrada. ¡Cuidado! para transformar una variable hay que conocer el margen dinámico de la misma (no podemos aplicar logaritmos a valores negativos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_transform = ['radius_se','perimeter_se','area_se','compactness_se']\n",
    "\n",
    "for feature in features_to_transform:\n",
    "    data[feature] = data[feature].apply(lambda x: np.log10(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pintamos de nuevo histogramas para cada clase\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "idx_0 =  data['diagnosis'] == 0\n",
    "idx_1 =  data['diagnosis'] == 1\n",
    "\n",
    "for i,feature in enumerate(data.columns.drop(['diagnosis'])):\n",
    "    plt.subplot(6,5,i+1)   \n",
    "    plt.hist(data.ix[idx_0,feature],normed=1, alpha=0.75,label='y=0')\n",
    "    plt.hist(data.ix[idx_1,feature],normed=1, facecolor='red', alpha=0.75,label='y=1')\n",
    "    plt.legend()\n",
    "    plt.title(feature)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preparamos los datos\n",
    "features = data.columns.drop(['diagnosis'])\n",
    "X = data[features].as_matrix()\n",
    "y = data['diagnosis'].as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "print('Datos train: ', X_train.shape)\n",
    "print('Datos test:  ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Árbol de decisión individual\n",
    "\n",
    "En primer lugar entrenamos un árbol individual para hacernos una idea de las prestaciones que alcanzamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "maxDepth = range(1,10)\n",
    "param_grid = {'max_depth': maxDepth }\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid=param_grid, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.plot(maxDepth,scores,'-o')\n",
    "plt.xlabel('max_depth',fontsize=16)\n",
    "plt.ylabel('10-Fold MSE')\n",
    "#plt.ylim((-1, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxDepthOptimo = grid.best_params_['max_depth']\n",
    "treeModel = DecisionTreeClassifier(max_depth=maxDepthOptimo).fit(X_train,y_train)\n",
    "\n",
    "print(\"Train: \",treeModel.score(X_train,y_train))\n",
    "print(\"Test: \",treeModel.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "tree_dot = export_graphviz(treeModel, out_file=None, feature_names=features, class_names=['B','M'],  \n",
    "                         filled=True, rounded=True,  special_characters=True)\n",
    "graph = graphviz.Source(tree_dot) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Bagging y Random Forest\n",
    "\n",
    "Los parámetros libres de Bagging y Random forest son dos:\n",
    "\n",
    "- Número de árboles construidos: aquí hemos de asegurarnos que la función de coste es estable para el número de árboles elegido\n",
    "- Complejidad de los mismos (normalmente max_depth o min_samples_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# grid search\n",
    "maxDepth = range(1,15)\n",
    "tuned_parameters = {'max_depth': maxDepth}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=0, n_estimators=200, max_features='sqrt'), param_grid=tuned_parameters,cv=10) \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.plot(maxDepth,scores,'-o')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('10-fold ACC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxDepthOptimo = grid.best_params_['max_depth']\n",
    "bagModel = RandomForestClassifier(max_depth=maxDepthOptimo,n_estimators=200,max_features='sqrt').fit(X_train,y_train)\n",
    "\n",
    "print(\"Train: \",bagModel.score(X_train,y_train))\n",
    "print(\"Test: \",bagModel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Importancia variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una propiedad muy interesante de los algoritmos basados en árboles es que podemos medir la importancia de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = bagModel.feature_importances_\n",
    "importances = importances / np.max(importances)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(range(X_train.shape[1]),importances[indices])\n",
    "plt.yticks(range(X_train.shape[1]),features[indices])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando este ranking, podemos hacer selección de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N,Nfeatures = X_train.shape\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=maxDepthOptimo,n_estimators=200,max_features='sqrt')\n",
    "kf  = KFold(n_splits=10, shuffle = True, random_state=1)\n",
    "cv_error = []\n",
    "cv_std = []\n",
    "\n",
    "for nfeatures in range(Nfeatures,0,-1):\n",
    "        \n",
    "    error_i = []\n",
    "    \n",
    "    for idxTrain, idxVal in kf.split(X_train):\n",
    "        \n",
    "        Xt = X_train[idxTrain,:]\n",
    "        yt = y_train[idxTrain]\n",
    "        Xv = X_train[idxVal,:]\n",
    "        yv = y_train[idxVal]\n",
    "        \n",
    "        rf.fit(Xt,yt)\n",
    "        \n",
    "        ranking = rf.feature_importances_\n",
    "        indices = np.argsort(ranking)[::-1] \n",
    "    \n",
    "        selected = indices[0:(Nfeatures-nfeatures+1)]\n",
    "        \n",
    "        Xs = Xt[:,selected]\n",
    "        \n",
    "        rf.fit(Xs,yt)\n",
    "        error = (1.0-rf.score(Xv[:,selected],yv))     \n",
    "        error_i.append(error) \n",
    "    \n",
    "    cv_error.append(np.mean(error_i))\n",
    "    cv_std.append(np.std(error_i))\n",
    "    \n",
    "    print('# features ' + str(len(selected)) + ' error ' + str(np.mean(error_i)) + ' +/- ' + str(np.std(error_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,Nfeatures+1,1),cv_error,'-o')\n",
    "plt.errorbar(range(1,Nfeatures+1,1), cv_error, yerr=cv_std, fmt='o')\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('CV error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Entrena un algoritmo de Random Forest con las características seleccionadas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ejemplo en regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cargamos datos\n",
    "house_data = pd.read_csv(\"./data/kc_house_data.csv\") # cargamos fichero\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "\n",
    "# convertir las variables en pies al cuadrado en metros al cuadrado \n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\n",
    "\n",
    "# renombramos\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# añadimos las nuevas variables\n",
    "house_data['years']            = 2017 - house_data['yr_built']\n",
    "house_data['bedrooms_squared'] = house_data['bedrooms'].apply(lambda x: x**2)\n",
    "house_data['bed_bath_rooms']   = house_data['bedrooms']*house_data['bathrooms']\n",
    "house_data['log_sqm_living']   = house_data['sqm_living'].apply(lambda x: np.log(x))\n",
    "house_data['lat_plus_long']    = house_data['lat']*house_data['long']\n",
    "\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convertimos el DataFrame al formato necesario para scikit-learn\n",
    "data = house_data.as_matrix() \n",
    "\n",
    "y = data[:,0:1]     # nos quedamos con la 1ª columna, price\n",
    "X = data[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "feature_names = house_data.columns[1:]\n",
    "\n",
    "# Dividimos los datos en entrenamiento y test (80 training, 20 test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state = 2)\n",
    "\n",
    "print('Datos entrenamiento: ', X_train.shape)\n",
    "print('Datos test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Entrena un árbol de decisión y devuelve las prestaciones para el conjunto de test\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Tu código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Entrena un algoritmo de random forest y devuelve las prestaciones para el conjunto de test\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Tu código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: ¿Qué características son las más relevantes? ¿Coinciden este ranking de características con las variables importante seleccionadas por el algoritmo Lasso?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu código aquí"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
