{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casos prácticos\n",
    "\n",
    "Este notebook, by [Felipe Alonso Atienza](www.linkedin.com/in/felipe-alonso-atienza)\n",
    "\n",
    "En este notebook vamos a abordar dos casos prácticos interesantes.\n",
    "\n",
    "1. Predicción (*forecasting*) de la demanda de bicicletas\n",
    "2. Clasificación multiclase de imágenes\n",
    "\n",
    "## Librerías y funciones\n",
    "\n",
    "Lo primero es cargar las librerías y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confmat):\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.5)\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.ylabel('true label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Predicción de la demanda de bicicletas\n",
    "\n",
    "El problema está descrito [aquí](https://christophm.github.io/interpretable-ml-book/bike-data.html), y los datos pueden descargarse en la [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset), aunque los hemos modificado un poco para hacer el problema más parecido al [*challenge*](https://www.kaggle.com/c/bike-sharing-demand/data) original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/bikes.csv',sep=';', decimal='.')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Interprete los datos y realiza tus primeras hipótesis sobre qué variables son de interés\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Elimine las columnas *instant*, *casual* y *registered*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 *Feature Engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con las fechas para crear algunas variables auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data['dteday'] = data['dteday'].apply(lambda x: datetime.strptime(x,'%d-%m-%Y'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['dteday'].apply(lambda x: x.year - 2011)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['dteday'].apply(lambda x: x.month)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekday'] = data['dteday'].apply(lambda x: x.isoweekday())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegados a este punto, podemos eliminar la variable *dteday*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['dteday'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Análisis de correlación\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Represente la variable *temp* vs *atemp*.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista está que son variables altamente correlacionadas. Podemos eliminar *temp*, ya que nos afecta más la sensación térmica que la temperatura real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['temp'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**(@home): Realice un análisis exploratorio exhaustivo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Codificación de variables categóricas\n",
    "\n",
    "Tenemos varias variables categóricas: *season*, *weathersit*, *month* y *weekday*. Cuando trabajamos con series temporales, es común crear variables *dummies* asociadas a cada una de las situaciones de las variables categóricas. Para ello, tenemos dos opciones:\n",
    "\n",
    "- [pd.get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html): se aplica directamente sobre el dataframe puede [utilizarse](https://towardsdatascience.com/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40) con strings directamente. La pega es que genera un nuevo dataframe que hay que agregar al original.\n",
    "- [OneHotEncoding()](http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features): sobre la matriz de datos (X), genera variables dummies sobre las columnas que se indique. Es muy sencillo de implementar, pero requiere que los datos hayan sido codificados previamente como enteros (labelEncoder()).\n",
    "\n",
    "La codificación \"dummy\" de variables categóricas en problemas de clasificación/regresión es opcional, como vimos anteriormente, pero en series temporales resulta adecuado para explicar el efecto de una situación temporal en la variable target.\n",
    "\n",
    "Vamos a utilizar los dos métodos para codificar las variables categóricas.  \n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**(@home): Prueba no realizar esta codificación \"dummy\" y entrena un modelo de machine learning para predecir la demanda de bicicletas.\n",
    "</div>\n",
    "\n",
    "Comenzamos por la variable *season*. Veamos qué hace *get_dummies()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data['season'], prefix = 'season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(data['season'], prefix = 'season')\n",
    "\n",
    "data = pd.concat([data,dummy],axis=1).drop(['season'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos los datos\n",
    "features = data.columns.drop(['cnt'])\n",
    "X = data[features].as_matrix()\n",
    "y = data['cnt'].as_matrix()\n",
    "\n",
    "print('Filas, columnas', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(categorical_features = [2,8,9], sparse=False, n_values=[3,12,7]) #weathersit, month, weekday\n",
    "X = enc.fit_transform(X)\n",
    "print('Filas, columnas', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 División train/test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1:\n",
    "offset = 182 # 0.25 of 731\n",
    "\n",
    "X_train = X[:-offset, :]\n",
    "y_train = y[:-offset]\n",
    "X_test  = X[-offset:, :]\n",
    "y_test  = y[-offset:]\n",
    "\n",
    "plt.plot(range(0,len(y_train)),y_train, label='train')\n",
    "plt.plot(range(len(y_train),len(y)),y_test,label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Búsqueda de parámetros libres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "alpha_vector = np.logspace(-4,4,20)\n",
    "param_grid = {'alpha': alpha_vector}\n",
    "\n",
    "grid = GridSearchCV(Lasso(), param_grid=param_grid, cv = tscv.split(X_train)).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "scores = grid.cv_results_['mean_test_score']\n",
    "std_scores = grid.cv_results_['std_test_score']\n",
    "plt.errorbar(np.log10(alpha_vector),scores,yerr=std_scores, fmt='o-',ecolor='g')\n",
    "plt.xlabel('log(alpha)',fontsize=16)\n",
    "plt.ylabel('CV MSE')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Métricas en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha_optimo = grid.best_params_['alpha']\n",
    "lasso = Lasso(alpha = alpha_optimo).fit(X_train,y_train)\n",
    "\n",
    "ytrainLasso = lasso.predict(X_train)\n",
    "ytestLasso  = lasso.predict(X_test)\n",
    "\n",
    "mseTrainModelLasso = mean_squared_error(y_train,ytrainLasso)\n",
    "mseTestModelLasso = mean_squared_error(y_test,ytestLasso)\n",
    "\n",
    "print('MSE Modelo Lasso (train): %0.3g' % mseTrainModelLasso)\n",
    "print('MSE Modelo Lasso (test) : %0.3g' % mseTestModelLasso)\n",
    "\n",
    "w = lasso.coef_\n",
    "for f,wi in zip(features,w):\n",
    "    print(f,wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Representa la predicción obtenida junto con la serie real (train+test)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clasificación multiclase de imágenes\n",
    "\n",
    "En este caso vamos a utilizar la famosa base de datos de [MNIST](http://yann.lecun.com/exdb/mnist/). Esta base de datos contiene\n",
    "\n",
    "* Training set: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 samples)\n",
    "* Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "* Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 samples)\n",
    "* Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n",
    "Estas imágenes se pueden descargar a partir del siguiente código (previamente hay que descargarse los archivos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import gzip\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte.gz' % kind)\n",
    "        \n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        lbpath.read(8)\n",
    "        buffer = lbpath.read()\n",
    "        labels = np.frombuffer(buffer, dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        imgpath.read(16)\n",
    "        buffer = imgpath.read()\n",
    "        images = np.frombuffer(buffer, \n",
    "                               dtype=np.uint8).reshape(\n",
    "            len(labels), 784).astype(np.float64)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('./data/mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obstante, sklearn tiene la base de datos incluida en sus datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "# rescale the data, use the traditional train/test split\n",
    "X, y = mnist.data / 255., mnist.target\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Fíjate que el conjunto de entrenamiento son los pixels de la imagen tal cual!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X train shape: \", X_train.shape)\n",
    "print (\"y train shape: \", y_train.shape)\n",
    "print (\"X test shape: \",  X_test.shape)\n",
    "print (\"y test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representamos algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomIds = np.random.randint(0,60000,10)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i,n in enumerate(randomIds):\n",
    "    img = X_train[n,:].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Entrena un modelo de regresión logística con C = 10 y calcula sus prestaciones en el conjunto de test. A lo mejor te resulta de utilidad revisar la [documentación](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Calcula y representa la matriz de confusión, ¿qué conclusiones puedes sacar?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
